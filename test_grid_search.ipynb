{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test avec différents paramétres : \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /users/nfs/Etu9/21200169/.local/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.9/site-packages (from pandas) (1.23.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /users/nfs/Etu9/21200169/.local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /users/nfs/Etu9/21200169/.local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[K     |████████████████████████████████| 773 kB 35.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/users/Etu9/21200169/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/users/Etu9/21200169/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import common as cmn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.model_selection._validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"./datasets/AFDpresidentutf8/corpus.tache1.learn.utf8\"\n",
    "alltxts,alllabs = cmn.load_pres(fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grid_search (parameters , scoring,preprocessors ,score_to_maximize):\n",
    "    pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=preprocessors)),\n",
    "    ('reg', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    [X_all_train, X_all_test, Y_train, y_test]  = train_test_split(alltxts, alllabs, test_size=0.3, random_state=10, shuffle=True)\n",
    "    \n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, parameters,scoring=scoring, refit=score_to_maximize, cv=5, n_jobs=-1, verbose=1)\n",
    "    \n",
    "    \n",
    "    grid_search.fit(X_all_train, Y_train)\n",
    "    \n",
    "    \n",
    "    print(\"Meilleurs paramètres trouvés:\")\n",
    "    print(grid_search.best_params_)\n",
    "    #_____________\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_all_test)\n",
    "    \n",
    "    smoothed_pred = gaussian_filter(y_pred,sigma=0.1)\n",
    "    \n",
    "    # Calcul des métriques de performance après le lissage\n",
    "\n",
    "    f1 = f1_score(y_test, smoothed_pred,  pos_label=-1)  # or 'macro' or 'weighted'\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, smoothed_pred)\n",
    "    accuracy = accuracy_score(y_test, (smoothed_pred > 0.5).astype(int))\n",
    "    \n",
    "    print(\"Performance après lissage:\")\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"AUC:\", roc_auc)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #_____________\n",
    "    # print(\"Scores:\")\n",
    "    # print(\"F1 Score:\", grid_search.cv_results_['mean_test_f1_score'])\n",
    "    # print(\"AUC:\", grid_search.cv_results_['mean_test_roc_auc'])\n",
    "    # print(\"Accuracy:\", grid_search.cv_results_['mean_test_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 :\n",
    "On essaye de penaliser notre classe minoritaire avec diffèrentes vaaleurs de pénaisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.9050743149094286\n",
      "AUC: 0.7827068919817048\n",
      "Accuracy: 0.8226892707849512\n"
     ]
    }
   ],
   "source": [
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.2,0.5],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [100000],\n",
    "    'reg__class_weight': [{1: 1, -1: w} for w in [1, 5, 10, 20]],  \n",
    "    'reg__max_iter': [10000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 1,10,100]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: ((cmn.suppression_chiffres(cmn.majuscules_en_marqueurs((cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 :\n",
    "Maximiser le f1 score de la classe minoritaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1.0, -1: 5.0}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.9040320473743795\n",
      "AUC: 0.782878231512926\n",
      "Accuracy: 0.8230427588609248\n"
     ]
    }
   ],
   "source": [
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.5],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [40000,100000],\n",
    "    'reg__class_weight': ['balanced',class_weights],  \n",
    "    'reg__max_iter': [10000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 1,10,100]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: (((cmn.majuscules_en_marqueurs((cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3 :\n",
    "Maximiser roc auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 1, 'reg__class_weight': 'balanced', 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 40000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.8680658364538884\n",
      "AUC: 0.8090931030390787\n",
      "Accuracy: 0.7726203953670644\n"
     ]
    }
   ],
   "source": [
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.5],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [40000,100000],\n",
    "    'reg__class_weight': ['balanced',class_weights],  \n",
    "    'reg__max_iter': [10000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 1,10,100]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: (((cmn.majuscules_en_marqueurs((cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'roc_auc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4:\n",
    "essayer de maximiser des deux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.9050743149094286\n",
      "AUC: 0.7827068919817048\n",
      "Accuracy: 0.8226892707849512\n"
     ]
    }
   ],
   "source": [
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.2,0.5],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [100000],\n",
    "    'reg__class_weight': [{1: 1, -1: w} for w in [1, 5, 10, 20]],  \n",
    "    'reg__max_iter': [10000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 1,10,100]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: ((cmn.suppression_chiffres(cmn.majuscules_en_marqueurs((cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POur tester sur mon pc :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.9050743149094286\n",
      "AUC: 0.7827068919817048\n",
      "Accuracy: 0.8226892707849512\n"
     ]
    }
   ],
   "source": [
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.2,0.5],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [100000],\n",
    "    'reg__class_weight': [{1: 1, -1: w} for w in [1, 5, 10, 20]],  \n",
    "    'reg__max_iter': [10000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 1,10,100]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: ((cmn.suppression_chiffres(cmn.majuscules_en_marqueurs((cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.6344735077129444\n",
      "AUC: 0.7827068919817048\n",
      "Accuracy: 0.8226892707849512\n"
     ]
    }
   ],
   "source": [
    "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.5],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [100000],\n",
    "    'reg__class_weight': [{1: 1, -1: 5} ],  \n",
    "    'reg__max_iter': [10000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 10]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: ((cmn.suppression_chiffres(cmn.majuscules_en_marqueurs(cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.45, 'tfidf__max_features': 90000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.6349419124218052\n",
      "AUC: 0.7831072171504867\n",
      "Accuracy: 0.82263121226196\n"
     ]
    }
   ],
   "source": [
    "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.45],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [90000],\n",
    "    'reg__class_weight': [{1: 1, -1: 5} ],  \n",
    "    'reg__max_iter': [10000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 10]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: ((cmn.suppression_chiffres(cmn.majuscules_en_marqueurs(cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 1000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.45, 'tfidf__max_features': 90000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.6361607142857143\n",
      "AUC: 0.7839748961092721\n",
      "Accuracy: 0.82263121226196\n"
     ]
    }
   ],
   "source": [
    "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.45],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [90000],\n",
    "    'reg__class_weight': [{1: 1, -1: 5} ],  \n",
    "    'reg__max_iter': [1000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 10]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: ((((cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 1000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.45, 'tfidf__max_features': 90000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.6361607142857143\n",
      "AUC: 0.7839748961092721\n",
      "Accuracy: 0.82263121226196\n"
     ]
    }
   ],
   "source": [
    "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.45],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [90000],\n",
    "    'reg__class_weight': [{1: 1, -1: 5} ],  \n",
    "    'reg__max_iter': [1000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 10]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: ((cmn.majuscules_en_marqueurs((cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 1000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.45, 'tfidf__max_features': 90000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.6296296296296295\n",
      "AUC: 0.7804017907837535\n",
      "Accuracy: 0.8217022758941013\n"
     ]
    }
   ],
   "source": [
    "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 10000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 100000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
    "class_weights = {\n",
    "    1: 1.0,  # Class 1, the majority class, gets weight 1.0 (default weight)\n",
    "    -1: 5.0  # Class -1, the minority class, gets weight 5.0\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.45],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [90000],\n",
    "    'reg__class_weight': [{1: 1, -1: 5} ],  \n",
    "    'reg__max_iter': [1000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 10]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: (cmn.lemmatization(cmn.majuscules_en_marqueurs((cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille initiale du vocabulaire avec des bigrammes : 325503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['de la',\n",
       " 'et de',\n",
       " 'la France',\n",
       " 'aujourd hui',\n",
       " 'et la',\n",
       " 'dans le',\n",
       " 'de notre',\n",
       " 'qu il',\n",
       " 'de nos',\n",
       " 'que nous',\n",
       " 'tous les',\n",
       " 'et les',\n",
       " 'dans la',\n",
       " 'Monsieur le',\n",
       " 'que la',\n",
       " 'que vous',\n",
       " 'dans les',\n",
       " 'et des',\n",
       " 'qui est',\n",
       " 'que les',\n",
       " 'et le',\n",
       " 'vous avez',\n",
       " 'est pas',\n",
       " 'et qui',\n",
       " 'pour la',\n",
       " 'le Président',\n",
       " 'que je',\n",
       " 'qui ont',\n",
       " 'ceux qui',\n",
       " 'est un',\n",
       " 'sur le',\n",
       " 'ce que',\n",
       " 'ce qui',\n",
       " 'de ce',\n",
       " 'de cette',\n",
       " 'il faut',\n",
       " 'de votre',\n",
       " 'du monde',\n",
       " 'qu elle',\n",
       " 'le monde',\n",
       " 'nous avons',\n",
       " 'que le',\n",
       " 'sur la',\n",
       " 'est une',\n",
       " 'par la',\n",
       " 'de ses',\n",
       " 'est la',\n",
       " 'pour les',\n",
       " 'les plus',\n",
       " 'Il faut',\n",
       " 'et je',\n",
       " 'qu ils',\n",
       " 'est le',\n",
       " 'doit être',\n",
       " 'la paix',\n",
       " 'toutes les',\n",
       " 'la vie',\n",
       " 'notre pays',\n",
       " 'de ces',\n",
       " 'par le',\n",
       " 'il est',\n",
       " 'de son',\n",
       " 'La France',\n",
       " 'sur les',\n",
       " 'de leur',\n",
       " 'de tous',\n",
       " 'qui sont',\n",
       " 'mais aussi',\n",
       " 'la République',\n",
       " 'plus de',\n",
       " 'Union européenne',\n",
       " 'qui se',\n",
       " 'est aussi',\n",
       " 'par les',\n",
       " 'entre les',\n",
       " 'nous devons',\n",
       " 'dans un',\n",
       " 'de Europe',\n",
       " 'pour le',\n",
       " 'que on',\n",
       " 'avec les',\n",
       " 'de plus',\n",
       " 'Il est',\n",
       " 'les Français',\n",
       " 'parce que',\n",
       " 'et du',\n",
       " 'ne peut',\n",
       " 'ont été',\n",
       " 'et en',\n",
       " 'pour que',\n",
       " 'le plus',\n",
       " 'les pays',\n",
       " 'qui nous',\n",
       " 'nos deux',\n",
       " 'et pour',\n",
       " 'avec le',\n",
       " 'la démocratie',\n",
       " 'de vous',\n",
       " 'le respect',\n",
       " 'en France']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessors = lambda x: ((cmn.majuscules_en_marqueurs((cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2),preprocessor=preprocessors)\n",
    "corpus_dicours = alltxts\n",
    "corpus_dicours_sparse_mat_bigram = vectorizer_bigram.fit_transform(corpus_dicours) # Output is a sparse matrix\n",
    "print(\"Taille initiale du vocabulaire avec des bigrammes :\",len(vectorizer_bigram.get_feature_names_out()))\n",
    "frequence = np.array(corpus_dicours_sparse_mat_bigram.sum(axis=0))[0]\n",
    "indices_tries = np.argsort(-frequence, kind='quicksort')\n",
    "\n",
    "# Trier les sommes des colonnes en utilisant les indices triés\n",
    "somme_colonnes_triees = [frequence[i] for i in indices_tries]\n",
    "somme_colonnes_triees\n",
    "\n",
    "bigrammes = vectorizer_bigram.get_feature_names_out()\n",
    "bigrammes_100_plus_frequents = [bigrammes[i] for i in indices_tries[:100] ]\n",
    "list(bigrammes_100_plus_frequents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille initiale du vocabulaire avec des bigrammes : 356716\n",
      "['de', 'la', 'et', 'le', 'les', 'des', 'est', 'que', 'qui', 'en', 'un', 'une', 'de la', 'pour', 'dans', 'du', 'vous', 'nous', 'au', 'plus', 'ce', 'il', 'pas', 'qu', 'je', 'par', 'notre', 'France', 'ne', 'sur', 'nos', 'avec', 'se', 'pays', 'sont', 'et de', 'aussi', 'Je', 'aux', 'cette', 'ont', 'être', 'leur', 'Il', 'tout', 'la France', 'tous', 'votre', 'son', 'La', 'même', 'elle', 'ou', 'ses', 'bien', 'entre', 'Europe', 'mais', 'hui', 'comme', 'on', 'ces', 'sa', 'doit', 'monde', 'faire', 'ai', 'aujourd', 'aujourd hui', 'et la', 'dans le', 'Et', 'faut', 'été', 'sans', 'fait', 'de notre', 'qu il', 'Mais', 'si', 'Nous', 'Le', 'où', 'leurs', 'avez', 'de nos', 'dire', 'Les', 'ils', 'très', 'deux', 'dont', 'peut', 'développement', 'que nous', 'tous les', 'et les', 'ensemble', 'dans la', 'Elle']\n",
      "[68623, 39750, 35180, 24366, 24362, 21495, 17247, 16081, 15568, 15473, 12257, 12012, 11919, 11033, 10962, 10639, 8131, 7735, 7571, 7266, 6585, 6393, 6312, 6241, 6128, 5546, 5218, 5201, 5157, 5133, 4718, 4652, 4492, 4420, 4386, 4372, 4300, 4158, 4093, 4014, 3952, 3798, 3661, 3643, 3378, 3378, 3317, 3175, 3137, 3110, 2858, 2825, 2802, 2791, 2768, 2645, 2607, 2597, 2582, 2578, 2542, 2509, 2501, 2460, 2441, 2391, 2344, 2212, 2212, 2142, 2119, 2096, 2088, 2070, 1981, 1974, 1934, 1908, 1893, 1873, 1855, 1850, 1840, 1789, 1757, 1752, 1716, 1705, 1698, 1694, 1648, 1623, 1615, 1607, 1601, 1600, 1581, 1571, 1570, 1560]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(1, 2),preprocessor=preprocessors)\n",
    "corpus_dicours = alltxts\n",
    "corpus_dicours_sparse_mat_bigram = vectorizer_bigram.fit_transform(corpus_dicours) # Output is a sparse matrix\n",
    "print(\"Taille initiale du vocabulaire avec des bigrammes :\",len(vectorizer_bigram.get_feature_names_out()))\n",
    "frequence = np.array(corpus_dicours_sparse_mat_bigram.sum(axis=0))[0]\n",
    "indices_tries = np.argsort(-frequence, kind='quicksort')\n",
    "\n",
    "# Trier les sommes des colonnes en utilisant les indices triés\n",
    "somme_colonnes_triees = [frequence[i] for i in indices_tries]\n",
    "somme_colonnes_triees\n",
    "\n",
    "bigrammes = vectorizer_bigram.get_feature_names_out()\n",
    "unigrammes_100_plus_frequents_12 = [bigrammes[i] for i in indices_tries[:100] ]\n",
    "print((unigrammes_100_plus_frequents_12))\n",
    "print((somme_colonnes_triees[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djeghali/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['France', 'aujourd', 'dans', 'hui', 'il', 'nos', 'notre', 'nous', 'qu', 'que'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 1000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_features': 90000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': ['de la', 'et de', 'la France', 'aujourd hui', 'et la', 'dans le', 'de notre', 'qu il', 'de nos', 'que nous', 'de', 'la', 'et', 'le', 'les']}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.6383259911894273\n",
      "AUC: 0.7879744546800008\n",
      "Accuracy: 0.8205411054342777\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parameters = {\n",
    "    # 'tfidf__max_df': [ 0.45],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [90000],\n",
    "    'tfidf__stop_words':[bigrammes_100_plus_frequents[:10]+unigrammes_100_plus_frequents[:5]] ,\n",
    "    'reg__class_weight': [{1: 1, -1: 5} ],  \n",
    "    'reg__max_iter': [1000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 10]\n",
    "\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: ((cmn.majuscules_en_marqueurs((cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__class_weight': {1: 1, -1: 5}, 'reg__max_iter': 1000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_features': 90000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.6348214285714285\n",
      "AUC: 0.7832235939583514\n",
      "Accuracy: 0.8224570366929865\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parameters = {\n",
    "    # 'tfidf__max_df': [ 0.45],\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [90000],\n",
    "    # 'tfidf__stop_words':[unigrammes_100_plus_frequents_12[:5]] ,\n",
    "    'reg__class_weight': [{1: 1, -1: 5} ],  \n",
    "    'reg__max_iter': [1000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 10]\n",
    "\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: ((cmn.majuscules_en_marqueurs((cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test ppti : avec over sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def grid_search_ov(parameters, scoring, preprocessors, score_to_maximize):\n",
    "    # Diviser les données en ensembles d'entraînement et de test\n",
    "    X_all_train, X_all_test, Y_train, y_test = train_test_split(alltxts, alllabs, test_size=0.3, random_state=10, shuffle=True)\n",
    "    \n",
    "    # Créer une instance de TfidfVectorizer avec les préprocesseurs spécifiés\n",
    "    tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessors)\n",
    "    \n",
    "    # Créer un pipeline avec TfidfVectorizer, suréchantillonnage SMOTE et LogisticRegression\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', tfidf_vectorizer),\n",
    "        ('smote', SMOTE()),  # Suréchantillonnage SMOTE\n",
    "        ('reg', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    # Initialiser la grille de recherche\n",
    "    grid_search = GridSearchCV(pipeline, parameters, scoring=scoring, refit=score_to_maximize, cv=5, n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # Effectuer la recherche sur grille\n",
    "    grid_search.fit(X_all_train, Y_train)\n",
    "    \n",
    "    # Afficher les meilleurs paramètres trouvés\n",
    "    print(\"Meilleurs paramètres trouvés:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    # Obtenir le meilleur modèle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Prédire les étiquettes sur l'ensemble de test avec le meilleur modèle\n",
    "    y_pred = best_model.predict(X_all_test)\n",
    "    \n",
    "    # Lissage des prédictions\n",
    "    smoothed_pred = gaussian_filter(y_pred, sigma=0.1)\n",
    "    \n",
    "    # Calculer les métriques de performance après lissage\n",
    "    f1 = f1_score(y_test, smoothed_pred, pos_label=-1)\n",
    "    roc_auc = roc_auc_score(y_test, smoothed_pred)\n",
    "    accuracy = accuracy_score(y_test, (smoothed_pred > 0.5).astype(int))\n",
    "    \n",
    "    print(\"Performance après lissage:\")\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"AUC:\", roc_auc)\n",
    "    print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__max_iter': 1000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.45, 'tfidf__max_features': 90000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.5801011804384486\n",
      "AUC: 0.7628558496430253\n",
      "Accuracy: 0.8044588945657223\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.45],\n",
    "\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [90000],\n",
    "    'reg__max_iter': [1000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 10]\n",
    "\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: (cmn.suppression_chiffres(cmn.majuscules_en_marqueurs((cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search_ov (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Meilleurs paramètres trouvés:\n",
      "{'reg__C': 10, 'reg__max_iter': 1000, 'reg__penalty': 'l2', 'reg__tol': 0.0001, 'tfidf__binary': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': 90000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "Performance après lissage:\n",
      "F1 Score: 0.5804311774461028\n",
      "AUC: 0.7661851220015414\n",
      "Accuracy: 0.8012076172782164\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': [ 0.5],\n",
    "\n",
    "    'tfidf__min_df': [2],\n",
    "    'tfidf__ngram_range': [ (1, 2)],\n",
    "    'tfidf__binary': [True],\n",
    "    'tfidf__max_features': [90000],\n",
    "    'reg__max_iter': [1000],\n",
    "    'reg__tol': [1e-4],\n",
    "    'reg__penalty': [ 'l2' ],\n",
    "    'reg__C': [ 10]\n",
    "\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score,pos_label=-1),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "preprocessors = lambda x: (cmn.suppression_chiffres(cmn.majuscules_en_marqueurs((cmn.suppression_ponctuation(cmn.suppression_balises_html((x)))))))\n",
    "\n",
    "grid_search_ov (parameters , scoring,preprocessors ,'f1_score')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
